- 图像识别 -> **CNN卷积神经网络**
- NLP自然语言处理 -> **RNN循环神经网络** 解决了seq2seq问题 输入输出语句长短不同
- Transformer与RNN都保留了**编码和解码**的结构
## 词嵌入
- “码”指的大概是 把语言中的形式的不同去除后，剩下的单纯的**语义关系**，其标准有：

1. 数字化
2. 数值体现语义之间的相对关系


> 对token进行数字化的两种极端情况，上述的第一点标准都能实现，但第二点都是极端的错误：

- **tokenizer标记器**（分词器）：每一个token都是不同的ID（一维）
- **onehot独热编码**：每一个token是二级制中的每一位（n维）

> 矩阵乘法实现词嵌入，编码进潜空间，解码出潜空间

- **潜空间**：没有形式上差别的语义空间，是连续的，因此遇到陌生的情况也能找到对应的值

## Word2Vec（词典）

> 一般的机器学习目标：得到一个模型，模型能够完成任务；**Word2Vec**得到的是嵌入矩阵，得到的是模型的参数

- Word2Vec**不需要激活函数**，隐藏层输出层的神经元做的就是向量求和与分解，没有非线性的需求

> 编码解码的过程大概是：输入token，经过矩阵编码得到词向量，而后解码再变成Token

- CBOW和skip-gram都是**自监督**的方法，前者根据上下文中的词来预测目标词，后者相反

## Transformer
### 注意力

1. 查询、键和值矩阵：

首先，输入被投影到查询（Query）、键（Key）和值（Value）矩阵上。
查询和键矩阵通过点积（dot product）相乘，得到一个得分矩阵。

2. 得分矩阵缩放：

得分矩阵通常会被除以键向量维度的平方根，以避免较大的点积导致的梯度消失或爆炸问题。

3. 应用Softmax：

在缩放后的得分矩阵上应用Softmax函数，将其转换为概率分布。这个操作确保了输出的注意力权重总和为1，可以视为每个位置上的相对重要性。

4. 加权和：

这些权重随后与值矩阵相乘，生成加权和表示，这个表示综合考虑了输入序列中各个部分的重要性。

### 位置编码
在Transformer架构中，位置编码为**加法实现（偏置系数）**
原始公式大概是一个傅里叶级数

绝对位置编码，让数据直接携带位置信息
相对位置编码，对注意力矩阵进行

> 注意力矩阵中的可以表示一个词和所有上下文的关系

**编码器encoder**注意力机制过程中会加入一个**掩码Masked**，将未来的词语对当前的词语影响降到无穷小
残差网络通过将输入与输出的残差相加，使得网络能学习到复杂的非线性映射，避免梯度爆炸消失的问题

> 架构中还有前馈神经网络和线性层 


