- 图像识别 -> **CNN卷积神经网络**
- NLP自然语言处理 -> **RNN循环神经网络** 解决了seq2seq问题 输入输出语句长短不同
- Transformer与RNN都保留了**编码和解码**的结构
## 词嵌入
- “码”指的大概是 把语言中的形式的不同去除后，剩下的单纯的**语义关系**，其标准有：

1. 数字化
2. 数值体现语义之间的相对关系


> 对token进行数字化的两种极端情况，上述的第一点标准都能实现，但第二点都是极端的错误：

- **tokenizer标记器**（分词器）：每一个token都是不同的ID（一维）
- **onehot独热编码**：每一个token是二级制中的每一位（n维）

> 矩阵乘法实现词嵌入，编码进潜空间，解码出潜空间

- **潜空间**：没有形式上差别的语义空间，是连续的，因此遇到陌生的情况也能找到对应的值

## Word2Vec（词典）

> 一般的机器学习目标：得到一个模型，模型能够完成任务；**Word2Vec**得到的是嵌入矩阵，得到的是模型的参数

- Word2Vec**不需要激活函数**，隐藏层输出层的神经元做的就是向量求和与分解，没有非线性的需求

> 编码解码的过程大概是：输入token，经过矩阵编码得到词向量，而后解码再变成Token

- CBOW和skip-gram都是**自监督**的方法，前者根据上下文中的词来预测目标词，后者相反

## Transformer
### 注意力

1. 查询、键和值矩阵：

首先，输入被投影到查询（Query）、键（Key）和值（Value）矩阵上。
查询和键矩阵通过点积（dot product）相乘，得到一个得分矩阵。

2. 得分矩阵缩放：

得分矩阵通常会被除以键向量维度的平方根，以避免较大的点积导致的梯度消失或爆炸问题。

3. 应用Softmax：

在缩放后的得分矩阵上应用Softmax函数，将其转换为概率分布。这个操作确保了输出的注意力权重总和为1，可以视为每个位置上的相对重要性。

4. 加权和：

这些权重随后与值矩阵相乘，生成加权和表示，这个表示综合考虑了输入序列中各个部分的重要性。

### 位置编码
在Transformer架构中，位置编码为**加法实现（偏置系数）**
原始公式大概是一个傅里叶级数

绝对位置编码，让数据直接携带位置信息
相对位置编码，对注意力矩阵进行

> 注意力矩阵中的可以表示一个词和所有上下文的关系

**编码器encoder**注意力机制过程中会加入一个**掩码Masked**，将未来的词语对当前的词语影响降到无穷小
残差网络通过将输入与输出的残差相加，使得网络能学习到复杂的非线性映射，避免梯度爆炸消失的问题

> 架构中还有前馈神经网络和线性层 


**输入层**
Tokenization: 输入文本首先会被分词器（tokenizer）分割成一系列的词汇单元（tokens）。
Embedding: 每个 token 都会被映射到一个固定维度的向量表示（embedding vector）。
Positional Encoding: 为了保持序列中单词的位置信息，Transformer 会在词嵌入上加上位置编码（positional encoding）。

**编码器 (Encoder)**
Multi-Head Attention: Transformer 的编码器包含多个自注意力（self-attention）层，每一层都是多头注意力（multi-head attention）的形式。多头注意力允许模型在不同表示子空间中并行地执行注意力操作。
Position-wise Feed-Forward Networks (FFNs): 每个注意力层之后都会跟着一个前馈神经网络（FFN），这个 FFN 对序列中的每一个位置单独且独立地应用相同的全连接神经网络。

**解码器 (Decoder)**
Masked Multi-Head Self-Attention: 解码器同样使用多头注意力，但是它的自注意力层会有一个特殊的掩码（mask），以确保当前位置的输出只依赖于先前的输出。
Multi-Head Attention with Encoder Outputs: 解码器还包括一个额外的注意力层，该层使用编码器的输出作为键（keys）和值（values），解码器层的输出作为查询（queries）。
Position-wise Feed-Forward Networks (FFNs): 解码器中也有 FFN 层，类似于编码器中的设计。

**输出层**
Final Linear Layer and Softmax: 最后的线性层将解码器的输出转换为适合特定任务的形式，例如对于分类任务，通常会接一个 softmax 函数产生概率分布。


